{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect 1 brand and see how the data inside that brand is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\grani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unrecognized model in jgranizo/sentimental. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, audio-spectrogram-transformer, autoformer, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deformable_detr, deit, depth_anything, deta, detr, dinat, dinov2, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, git, glm, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, graphormer, grounding-dino, groupvit, hiera, hubert, ibert, idefics, idefics2, idefics3, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rwkv, sam, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, siglip, siglip_vision_model, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, time_series_transformer, timesformer, timm_backbone, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zoedepth",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSequenceClassification\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the tokenizer and model from Hugging Face\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjgranizo/sentimental\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjgranizo/sentimental\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\grani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:877\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    875\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfor_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_dict)\n\u001b[0;32m    876\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 877\u001b[0m         config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m config_tokenizer_class \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtokenizer_class\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n",
      "File \u001b[1;32mc:\\Users\\grani\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:1049\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m   1046\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(pretrained_model_name_or_path):\n\u001b[0;32m   1047\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m CONFIG_MAPPING[pattern]\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[1;32m-> 1049\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1050\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized model in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1051\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould have a `model_type` key in its \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, or contain one of the following strings \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min its name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(CONFIG_MAPPING\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1053\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Unrecognized model in jgranizo/sentimental. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: albert, align, altclip, audio-spectrogram-transformer, autoformer, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, blenderbot, blenderbot-small, blip, blip-2, bloom, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, conditional_detr, convbert, convnext, convnextv2, cpmant, ctrl, cvt, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deformable_detr, deit, depth_anything, deta, detr, dinat, dinov2, distilbert, donut-swin, dpr, dpt, efficientformer, efficientnet, electra, encodec, encoder-decoder, ernie, ernie_m, esm, falcon, falcon_mamba, fastspeech2_conformer, flaubert, flava, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, git, glm, glpn, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gptj, gptsan-japanese, granite, granitemoe, graphormer, grounding-dino, groupvit, hiera, hubert, ibert, idefics, idefics2, idefics3, imagegpt, informer, instructblip, instructblipvideo, jamba, jetmoe, jukebox, kosmos-2, layoutlm, layoutlmv2, layoutlmv3, led, levit, lilt, llama, llava, llava_next, llava_next_video, llava_onevision, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, mgp-str, mimi, mistral, mixtral, mllama, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, owlv2, owlvit, paligemma, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, persimmon, phi, phi3, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rwkv, sam, seamless_m4t, seamless_m4t_v2, segformer, seggpt, sew, sew-d, siglip, siglip_vision_model, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, table-transformer, tapas, time_series_transformer, timesformer, timm_backbone, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vits, vivit, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xmod, yolos, yoso, zamba, zoedepth"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load the tokenizer and model from Hugging Face\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jgranizo/sentimental\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"jgranizo/sentimental\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grani\\AppData\\Local\\Temp\\ipykernel_14824\\3768352234.py:19: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  date_limit = datetime.utcnow() - timedelta(days=days_limit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching posts about `Apple` in r/technology\n",
      "\n",
      "Fetching posts about `Apple` in r/gadgets\n",
      "\n",
      "Fetching posts about `Apple` in r/stocks\n",
      "\n",
      "--- Post Card ---\n",
      "Title: Warren Buffett’s Berkshire Hathaway sold ~$14 billion worth of Apple this quarter. Cash pile hits record ~$325 billion.\n",
      "Score: 1010 | Comments: 328\n",
      "Created: 2024-11-02 08:19:31 | Subreddit: stocks\n",
      "Author: themagicalpanda\n",
      "URL: https://www.reddit.com/r/stocks/comments/1ghv5qo/warren_buffetts_berkshire_hathaway_sold_14/\n",
      "Content Preview: Berkshires cash on hand hits a record $325 Billion\n",
      "\n",
      "Currently owns ~$69.9 Billion worth of Apple $AAPL as of the end of Q3 down from $84.2B worth as of the end of Q2\n",
      "\n",
      "No brk.a or brk.b shares repurchase in Q3\n",
      "\n",
      "Posts Q3 revenue of $92.9B compared to $93.2B YoY\n",
      "\n",
      "Operating earnings fell 6% from a year earlier to $10.09 billion as insurance underwriting earnings slumped\n",
      "\n",
      "#\n",
      "\n",
      ">**It is being widely reported that Berkshire's operating earnings declined 6.2% in the quarter. No. The reported figures include unrealized currency losses in 2024 and gains in 2023 on BRK's debt denominated in foreign currencies, largely yen. Earnings really grew 8.5% in Q3 to $11.2 billion against $10.3 billion and were up 18.0% for the nine months to $33.0 billion from $28.0 billion. Just as short-term movements in equity prices can distort the reporting of operating profits, so to do currency swings on debt outstanding. Both should be excluded from an analysis of operating and economic profit.**\n",
      "\n",
      "Per Christopher Bloomstran on X \n",
      "\n",
      "Top Comments:\n",
      "Comment 1: His cash position is about 2x Blackrock market cap.\n",
      "Comment 2: Imagine having so much cash you could literally set a few million dollars on fire and no one would notice.\n",
      "Comment 3: What is Buffett cooking?\n",
      "Comment 4: That is a lot of cash.\n",
      "\n",
      "So much speculation from succession, new administration, new tax fears to market being overvalued.\n",
      "\n",
      "Berkshire has been acquiring more insurance companies and buying stocks like CB which is also insurance.\n",
      "\n",
      "Also Berkshire energy is growing as well.\n",
      "Comment 5: And AAPL bought back 95b worth of shares this year\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Initialize Reddit API client with your credentials\n",
    "reddit = praw.Reddit(\n",
    "    client_id='ZPr2EshdHCAQUHQ1B4LRdQ',\n",
    "    client_secret='D_pSNQdQpLFu0xkme74Sket1zGUorg',\n",
    "    user_agent='BrandDataCollector/1.0 by u/jgran'\n",
    ")\n",
    "\n",
    "\n",
    "# Define criteria\n",
    "brand_name = \"Apple\"\n",
    "min_score = 100  # Minimum score to filter high-upvote posts\n",
    "min_comments = 50  # Minimum number of comments for high engagement\n",
    "days_limit = 7  # Only get posts from the last 7 days\n",
    "\n",
    "# Calculate the Unix timestamp for the date limit\n",
    "date_limit = datetime.utcnow() - timedelta(days=days_limit)\n",
    "timestamp_limit = date_limit.timestamp()\n",
    "\n",
    "# Search for posts mentioning the brand in relevant subreddits\n",
    "subreddits = [\"technology\", \"gadgets\", \"stocks\"]\n",
    "posts_data = []\n",
    "subreddit_dictionary = {}\n",
    "\n",
    "for subreddit_name in subreddits:\n",
    "    print(f\"\\nFetching posts about `{brand_name}` in r/{subreddit_name}\")\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    \n",
    "    for post in subreddit.search(brand_name, limit=10):  # Adjust limit as needed\n",
    "        # Filter posts based on criteria\n",
    "        if (post.score >= min_score and\n",
    "            post.num_comments >= min_comments and\n",
    "            post.created_utc >= timestamp_limit):\n",
    "\n",
    "            # Store post attributes in a dictionary\n",
    "            post_info = {\n",
    "                \"Title\": post.title,\n",
    "                \"Score\": post.score,\n",
    "                \"Comments\": post.num_comments,\n",
    "                \"Created\": datetime.fromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                \"Subreddit\": post.subreddit.display_name,\n",
    "                \"Author\": post.author,\n",
    "                \"URL\": post.url,\n",
    "                \"Content\": post.selftext  # Short preview of the post content\n",
    "            }\n",
    "             # Set comment sort order to 'top' and fetch top comments\n",
    "            post.comment_sort = 'top'\n",
    "            post.comments.replace_more(limit=0)  # Avoids collapsed comments\n",
    "            top_comments = [comment.body for comment in post.comments[:5]]\n",
    "\n",
    "            # Attach top comments to post_info\n",
    "            post_info[\"Top_Comments\"] = top_comments\n",
    "         \n",
    "            # Display each post as a \"card\"\n",
    "            print(\"\\n--- Post Card ---\")\n",
    "            print(f\"Title: {post_info['Title']}\")\n",
    "            print(f\"Score: {post_info['Score']} | Comments: {post_info['Comments']}\")\n",
    "            print(f\"Created: {post_info['Created']} | Subreddit: {post_info['Subreddit']}\")\n",
    "            print(f\"Author: {post_info['Author']}\")\n",
    "            print(f\"URL: {post_info['URL']}\")\n",
    "            print(f\"Content Preview: {post_info['Content']}\")\n",
    "            print(\"\\nTop Comments:\")\n",
    "            for i, comment in enumerate(post_info[\"Top_Comments\"], 1):\n",
    "                print(f\"Comment {i}: {comment}\")  # Show first 100 characters for brevity\n",
    "                if post_info['Title'] not in subreddit_dictionary:\n",
    "                    subreddit_dictionary[post_info['Title']] = {}\n",
    "                subreddit_dictionary[post_info['Title']][comment] = 0\n",
    "            # Store post and comments for further analysis\n",
    "            posts_data.append(post_info)\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Warren Buffett’s Berkshire Hathaway sold ~$14 billion worth of Apple this quarter. Cash pile hits record ~$325 billion.': {'His cash position is about 2x Blackrock market cap.': 0, 'Imagine having so much cash you could literally set a few million dollars on fire and no one would notice.': 0, 'What is Buffett cooking?': 0, 'That is a lot of cash.\\n\\nSo much speculation from succession, new administration, new tax fears to market being overvalued.\\n\\nBerkshire has been acquiring more insurance companies and buying stocks like CB which is also insurance.\\n\\nAlso Berkshire energy is growing as well.': 0, 'And AAPL bought back 95b worth of shares this year': 0}}\n",
      "This is a subreddit title :  Warren Buffett’s Berkshire Hathaway sold ~$14 billion worth of Apple this quarter. Cash pile hits record ~$325 billion.\n",
      "\n",
      "This is a comment\n",
      " His cash position is about 2x Blackrock market cap. 0 \n",
      "\n",
      "\n",
      "This is a comment\n",
      " Imagine having so much cash you could literally set a few million dollars on fire and no one would notice. 0 \n",
      "\n",
      "\n",
      "This is a comment\n",
      " What is Buffett cooking? 0 \n",
      "\n",
      "\n",
      "This is a comment\n",
      " That is a lot of cash.\n",
      "\n",
      "So much speculation from succession, new administration, new tax fears to market being overvalued.\n",
      "\n",
      "Berkshire has been acquiring more insurance companies and buying stocks like CB which is also insurance.\n",
      "\n",
      "Also Berkshire energy is growing as well. 0 \n",
      "\n",
      "\n",
      "This is a comment\n",
      " And AAPL bought back 95b worth of shares this year 0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(subreddit_dictionary)\n",
    "\n",
    "for title in subreddit_dictionary.keys():\n",
    "    print(\"This is a subreddit title : \",title)\n",
    "    for comment in subreddit_dictionary[title]:\n",
    "       print(\"\\nThis is a comment\\n\",comment,subreddit_dictionary[title][comment],\"\\n\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\grani\\AppData\\Local\\Temp\\ipykernel_14824\\2737553353.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('checkpoint.pth', map_location='cpu')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "checkpoint = torch.load('checkpoint.pth', map_location='cpu')\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a subreddit title :  Warren Buffett’s Berkshire Hathaway sold ~$14 billion worth of Apple this quarter. Cash pile hits record ~$325 billion.\n",
      "This is the sentimental value of subreddit title:  Negative\n",
      "\n",
      "This is a comment\n",
      " His cash position is about 2x Blackrock market cap. \n",
      " Its sentimental value is:  Neutral \n",
      "\n",
      "\n",
      "This is a comment\n",
      " Imagine having so much cash you could literally set a few million dollars on fire and no one would notice. \n",
      " Its sentimental value is:  Neutral \n",
      "\n",
      "\n",
      "This is a comment\n",
      " What is Buffett cooking? \n",
      " Its sentimental value is:  Negative \n",
      "\n",
      "\n",
      "This is a comment\n",
      " That is a lot of cash.\n",
      "\n",
      "So much speculation from succession, new administration, new tax fears to market being overvalued.\n",
      "\n",
      "Berkshire has been acquiring more insurance companies and buying stocks like CB which is also insurance.\n",
      "\n",
      "Also Berkshire energy is growing as well. \n",
      " Its sentimental value is:  Positive \n",
      "\n",
      "\n",
      "This is a comment\n",
      " And AAPL bought back 95b worth of shares this year \n",
      " Its sentimental value is:  Positive \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predict_sentence(sentence, model, tokenizer, label_mapping, max_len=128):\n",
    "    # Tokenize and encode the sentence\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        sentence,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # Move tensors to the appropriate device\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # Get the predicted class\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    # Map the predicted class to label\n",
    "    predicted_label = label_mapping[predicted_class]\n",
    "\n",
    "    return predicted_label\n",
    "\n",
    "# Example usage\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "label_mapping = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}  # Adjust as needed\n",
    "sentence = \"Tech companies spent all this money on AI only to realize after they have no idea how to actually make money off it.\"\n",
    "\n",
    "\n",
    "for title in subreddit_dictionary.keys():\n",
    "    print(\"This is a subreddit title : \",title)\n",
    "    print(\"This is the sentimental value of subreddit title: \",predict_sentence(title, model, tokenizer, label_mapping))\n",
    "    for comment in subreddit_dictionary[title]:\n",
    "       current_sentimental_value=predict_sentence(comment, model, tokenizer, label_mapping)\n",
    "       subreddit_dictionary[title][comment] = current_sentimental_value\n",
    "       print(\"\\nThis is a comment\\n\",comment,\"\\n Its sentimental value is: \",predict_sentence(comment, model, tokenizer, label_mapping) ,\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
